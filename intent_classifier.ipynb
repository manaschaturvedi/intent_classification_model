{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/manas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Sentences          Intent\n",
      "0                                      Need help pleese  commonQ.assist\n",
      "1                                             Need help  commonQ.assist\n",
      "2                                      I need some info  commonQ.assist\n",
      "3                                     Will you help me?  commonQ.assist\n",
      "4                                 What else can you do?  commonQ.assist\n",
      "...                                                 ...             ...\n",
      "1108  borrowed amount can be used for which of the p...  faq.borrow_use\n",
      "1109  borrowed amount given can used by me for what ...  faq.borrow_use\n",
      "1110  borrowed amount given can used by me for which...  faq.borrow_use\n",
      "1111  borrowed money can be used for which of the pu...  faq.borrow_use\n",
      "1112  borrow money given can used by me for what rea...  faq.borrow_use\n",
      "\n",
      "[1113 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset.csv', encoding = \"latin1\", names=[\"Sentences\", \"Intent\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faq.approval_time', 'faq.biz_simpler', 'faq.biz_new', 'commonQ.how', 'faq.application_process', 'faq.borrow_limit', 'faq.borrow_use', 'commonQ.query', 'faq.banking_option_missing', 'faq.apply_register', 'commonQ.bot', 'commonQ.name', 'faq.bad_service', 'faq.biz_category_missing', 'commonQ.wait', 'commonQ.just_details', 'commonQ.not_giving', 'faq.aadhaar_missing', 'contact.contact', 'commonQ.assist', 'faq.address_proof'} 21\n"
     ]
    }
   ],
   "source": [
    "unique_intents = set(list(dataset['Intent']))\n",
    "print(unique_intents, len(unique_intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Need help pleese', 'Need help', 'I need some info', 'Will you help me?', 'What else can you do?']\n"
     ]
    }
   ],
   "source": [
    "sentences = list(dataset['Sentences'])\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['need', 'help', 'pleese'], ['need', 'help'], ['i', 'need', 'some', 'info'], ['will', 'you', 'help', 'me'], ['what', 'else', 'can', 'you', 'do']]\n"
     ]
    }
   ],
   "source": [
    "# DATA CLEANING\n",
    "# Before we can feed our data into an algorithm to train our model, we would have to clean the raw data.\n",
    "\n",
    "# Step 1: remove any punctuation marks or special characters (if any) from each sentence\n",
    "# Step 2: tokenize each sentence i.e. convert each sentence into a list of words\n",
    "# Tokenization is the process by which big quantity of text is divided into smaller parts called tokens\n",
    "# Step 3: Apply Lemmatization on each word (token) in the sentence\n",
    "# Lemmatization is the process of deriving the actual root lemma (word) from a given word\n",
    "# for example: apologies => apology, horses => horse\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cleaned_sentences = []\n",
    "for s in sentences:\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s) # replace any special characters with an empty space\n",
    "    w = word_tokenize(clean)\n",
    "    cleaned_sentences.append([lemmatizer.lemmatize(i.lower()) for i in w])\n",
    "print(cleaned_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
