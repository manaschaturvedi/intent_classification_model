{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/manas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Sentences          Intent\n",
      "0                                      Need help pleese  commonQ.assist\n",
      "1                                             Need help  commonQ.assist\n",
      "2                                      I need some info  commonQ.assist\n",
      "3                                     Will you help me?  commonQ.assist\n",
      "4                                 What else can you do?  commonQ.assist\n",
      "...                                                 ...             ...\n",
      "1108  borrowed amount can be used for which of the p...  faq.borrow_use\n",
      "1109  borrowed amount given can used by me for what ...  faq.borrow_use\n",
      "1110  borrowed amount given can used by me for which...  faq.borrow_use\n",
      "1111  borrowed money can be used for which of the pu...  faq.borrow_use\n",
      "1112  borrow money given can used by me for what rea...  faq.borrow_use\n",
      "\n",
      "[1113 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset.csv', encoding = \"latin1\", names=[\"Sentences\", \"Intent\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faq.aadhaar_missing', 'faq.biz_new', 'commonQ.assist', 'faq.apply_register', 'faq.application_process', 'commonQ.not_giving', 'commonQ.just_details', 'commonQ.query', 'faq.borrow_use', 'faq.biz_category_missing', 'faq.borrow_limit', 'commonQ.name', 'faq.banking_option_missing', 'contact.contact', 'faq.address_proof', 'commonQ.bot', 'faq.approval_time', 'faq.bad_service', 'commonQ.how', 'faq.biz_simpler', 'commonQ.wait'} 21\n"
     ]
    }
   ],
   "source": [
    "intents = dataset['Intent'].tolist()\n",
    "unique_intents = set(list(dataset['Intent']))\n",
    "print(unique_intents, len(unique_intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Need help pleese', 'Need help', 'I need some info', 'Will you help me?', 'What else can you do?']\n"
     ]
    }
   ],
   "source": [
    "sentences = list(dataset['Sentences'])\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['need', 'help', 'pleese'], ['need', 'help'], ['i', 'need', 'some', 'info'], ['will', 'you', 'help', 'me'], ['what', 'else', 'can', 'you', 'do']]\n"
     ]
    }
   ],
   "source": [
    "# DATA CLEANING\n",
    "# Before we can feed our data into an algorithm to train our model, we would have to clean the raw data.\n",
    "\n",
    "# Step 1: remove any punctuation marks or special characters (if any) from each sentence\n",
    "# Step 2: tokenize each sentence i.e. convert each sentence into a list of words\n",
    "# Tokenization is the process by which big quantity of text is divided into smaller parts called tokens\n",
    "# Words are called tokens and the process of splitting text into tokens is called tokenization.\n",
    "# Step 3: Apply Lemmatization on each word (token) in the sentence\n",
    "# Lemmatization is the process of deriving the actual root lemma (word) from a given word\n",
    "# for example: apologies => apology, horses => horse\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cleaned_sentences = []\n",
    "for s in sentences:\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s) # replace any special characters with an empty space\n",
    "    w = word_tokenize(clean)\n",
    "    cleaned_sentences.append([lemmatizer.lemmatize(i.lower()) for i in w])\n",
    "print(cleaned_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 461\n",
      "Length of the longest sentence: 28\n"
     ]
    }
   ],
   "source": [
    "# INPUT ENCODING: converting our input messages into numeric values understandable by our ML algorithm\n",
    "# It is popular to represent a document as a sequence of integer values, where each word in the document \n",
    "# is represented as a unique integer.\n",
    "token = Tokenizer(filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "token.fit_on_texts(cleaned_sentences)\n",
    "max_sentence_length = len(max(cleaned_sentences, key=len))\n",
    "print('Vocab size:', len(token.word_index)) # word_index: A dict of words and their uniquely assigned integers.\n",
    "print('Length of the longest sentence:', max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25, 74, 316], [25, 74], [1, 25, 194, 176], [54, 10, 74, 16], [9, 261, 4, 10, 30]]\n"
     ]
    }
   ],
   "source": [
    "encoded_doc = token.texts_to_sequences(cleaned_sentences)\n",
    "print(encoded_doc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25  74 316 ...   0   0   0]\n",
      " [ 25  74   0 ...   0   0   0]\n",
      " [  1  25 194 ...   0   0   0]\n",
      " ...\n",
      " [ 59  28 129 ...   0   0   0]\n",
      " [ 59  44   4 ...   0   0   0]\n",
      " [ 81  44 129 ...   0   0   0]]\n",
      "Shape of features: (1113, 28)\n"
     ]
    }
   ],
   "source": [
    "padded_doc = pad_sequences(encoded_doc, maxlen = max_sentence_length, padding = \"post\")\n",
    "print(padded_doc)\n",
    "print(\"Shape of features:\",padded_doc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faq.aadhaar_missing': 1, 'faq.biz_new': 2, 'commonq.assist': 3, 'faq.apply_register': 4, 'faq.application_process': 5, 'commonq.not_giving': 6, 'commonq.just_details': 7, 'commonq.query': 8, 'faq.borrow_use': 9, 'faq.biz_category_missing': 10, 'faq.borrow_limit': 11, 'commonq.name': 12, 'faq.banking_option_missing': 13, 'contact.contact': 14, 'faq.address_proof': 15, 'commonq.bot': 16, 'faq.approval_time': 17, 'faq.bad_service': 18, 'commonq.how': 19, 'faq.biz_simpler': 20, 'commonq.wait': 21}\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT ENCODING\n",
    "# changed the filters here by removing . and _ so as to preserve the labels\n",
    "output_token = Tokenizer(filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n",
    "output_token.fit_on_texts(unique_intents)\n",
    "print(output_token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3], [3], [3], [3], [3]]\n"
     ]
    }
   ],
   "source": [
    "encoded_output = output_token.texts_to_sequences(intents)\n",
    "print(encoded_output[:5]) # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n",
      "Shape of label: (1113, 1)\n"
     ]
    }
   ],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)\n",
    "print(encoded_output[:5]) # numpy array\n",
    "print('Shape of label:', encoded_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manas/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding the output\n",
    "o = OneHotEncoder(sparse = False)\n",
    "output_one_hot = o.fit_transform(encoded_output)\n",
    "print(output_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
