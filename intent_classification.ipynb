{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    df = pd.read_csv(filename, encoding = \"latin1\", names = [\"Sentence\", \"Intent\"])\n",
    "    print(df.head())\n",
    "    intents = df[\"Intent\"]\n",
    "    unique_intents = list(set(intents))\n",
    "    sentences = list(df[\"Sentence\"])\n",
    "\n",
    "    return (intents, unique_intents, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Sentence          Intent\n",
      "0       Need help pleese  commonQ.assist\n",
      "1              Need help  commonQ.assist\n",
      "2       I need some info  commonQ.assist\n",
      "3      Will you help me?  commonQ.assist\n",
      "4  What else can you do?  commonQ.assist\n"
     ]
    }
   ],
   "source": [
    "intents, unique_intents, sentences = load_dataset(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Need help pleese', 'Need help', 'I need some info', 'Will you help me?', 'What else can you do?']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/manas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/manas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\") # downloading corpus of stopwords\n",
    "nltk.download(\"punkt\") # reference: https://www.nltk.org/_modules/nltk/tokenize/punkt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer() # reference: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "      words = []\n",
    "      for s in sentences:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        # TODO: apply stemming before appending into words\n",
    "        words.append([i.lower() for i in w])\n",
    "\n",
    "      return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113\n",
      "[['need', 'help', 'pleese'], ['need', 'help']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(sentences)\n",
    "print(len(cleaned_words))\n",
    "print(cleaned_words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references:\n",
    "# https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "# https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
    "\n",
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    token = Tokenizer(filters = filters)\n",
    "    token.fit_on_texts(words)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(words):\n",
    "    return(len(max(words, key = len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 492 and Maximum length = 28\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = max_length(cleaned_words)\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))\n",
    "# print(word_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "    return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)\n",
    "# print(encoded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "    return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25  77 332 ...   0   0   0]\n",
      " [ 25  77   0 ...   0   0   0]\n",
      " [  1  25 198 ...   0   0   0]\n",
      " ...\n",
      " [ 59  28 133 ...   0   0   0]\n",
      " [ 59  42   4 ...   0   0   0]\n",
      " [ 84  42 133 ...   0   0   0]]\n",
      "Shape of padded docs =  (1113, 28)\n"
     ]
    }
   ],
   "source": [
    "padded_doc = padding_doc(encoded_doc, max_length)\n",
    "print(padded_doc)\n",
    "print(\"Shape of padded docs = \",padded_doc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer with filter changed\n",
    "output_tokenizer = create_tokenizer(unique_intents, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'commonq.not_giving': 1, 'commonq.name': 2, 'faq.address_proof': 3, 'faq.apply_register': 4, 'faq.application_process': 5, 'faq.biz_new': 6, 'faq.biz_category_missing': 7, 'commonq.how': 8, 'commonq.wait': 9, 'contact.contact': 10, 'commonq.query': 11, 'commonq.bot': 12, 'commonq.assist': 13, 'faq.biz_simpler': 14, 'faq.banking_option_missing': 15, 'faq.borrow_limit': 16, 'faq.aadhaar_missing': 17, 'faq.borrow_use': 18, 'faq.bad_service': 19, 'faq.approval_time': 20, 'commonq.just_details': 21}\n"
     ]
    }
   ],
   "source": [
    "print(output_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13], [13], [13], [13], [13]]\n"
     ]
    }
   ],
   "source": [
    "encoded_output = encoding_doc(output_tokenizer, intents)\n",
    "print(encoded_output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1113, 1)\n"
     ]
    }
   ],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)\n",
    "print(encoded_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(encode):\n",
    "      o = OneHotEncoder(sparse = False)\n",
    "      return(o.fit_transform(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1113, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manas/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "output_one_hot = one_hot(encoded_output)\n",
    "print(output_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (890, 28) and train_Y = (890, 21)\n",
      "Shape of val_X = (223, 28) and val_Y = (223, 21)\n"
     ]
    }
   ],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)\n",
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "      model = Sequential()\n",
    "      model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "      model.add(Bidirectional(LSTM(128)))\n",
    "      #  model.add(LSTM(128))\n",
    "      model.add(Dense(32, activation = \"relu\"))\n",
    "      model.add(Dropout(0.5))\n",
    "      model.add(Dense(21, activation = \"softmax\"))\n",
    "\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 28, 128)           62976     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 21)                693       \n",
      "=================================================================\n",
      "Total params: 335,061\n",
      "Trainable params: 272,085\n",
      "Non-trainable params: 62,976\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 890 samples, validate on 223 samples\n",
      "Epoch 1/100\n",
      "890/890 [==============================] - 4s 4ms/step - loss: 2.9582 - acc: 0.0843 - val_loss: 2.7728 - val_acc: 0.1345\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.77283, saving model to model.h5\n",
      "Epoch 2/100\n",
      "890/890 [==============================] - 1s 850us/step - loss: 2.8746 - acc: 0.1112 - val_loss: 2.7185 - val_acc: 0.1794\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.77283 to 2.71847, saving model to model.h5\n",
      "Epoch 3/100\n",
      "890/890 [==============================] - 1s 881us/step - loss: 2.7907 - acc: 0.1562 - val_loss: 2.6382 - val_acc: 0.2018\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.71847 to 2.63819, saving model to model.h5\n",
      "Epoch 4/100\n",
      "890/890 [==============================] - 1s 805us/step - loss: 2.7052 - acc: 0.1674 - val_loss: 2.5380 - val_acc: 0.2152\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.63819 to 2.53801, saving model to model.h5\n",
      "Epoch 5/100\n",
      "890/890 [==============================] - 1s 708us/step - loss: 2.5694 - acc: 0.2202 - val_loss: 2.4741 - val_acc: 0.2646\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.53801 to 2.47412, saving model to model.h5\n",
      "Epoch 6/100\n",
      "890/890 [==============================] - 1s 844us/step - loss: 2.5182 - acc: 0.2225 - val_loss: 2.3678 - val_acc: 0.2377\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.47412 to 2.36785, saving model to model.h5\n",
      "Epoch 7/100\n",
      "890/890 [==============================] - 1s 635us/step - loss: 2.3839 - acc: 0.2708 - val_loss: 2.2588 - val_acc: 0.3408\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.36785 to 2.25881, saving model to model.h5\n",
      "Epoch 8/100\n",
      "890/890 [==============================] - 1s 652us/step - loss: 2.2795 - acc: 0.3000 - val_loss: 2.1747 - val_acc: 0.3722\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.25881 to 2.17467, saving model to model.h5\n",
      "Epoch 9/100\n",
      "890/890 [==============================] - 1s 646us/step - loss: 2.1755 - acc: 0.3236 - val_loss: 2.1845 - val_acc: 0.4350\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.17467\n",
      "Epoch 10/100\n",
      "890/890 [==============================] - 1s 635us/step - loss: 2.1364 - acc: 0.3539 - val_loss: 2.0143 - val_acc: 0.4574\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.17467 to 2.01428, saving model to model.h5\n",
      "Epoch 11/100\n",
      "890/890 [==============================] - 1s 668us/step - loss: 2.0193 - acc: 0.3831 - val_loss: 1.8886 - val_acc: 0.4843\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.01428 to 1.88858, saving model to model.h5\n",
      "Epoch 12/100\n",
      "890/890 [==============================] - 1s 636us/step - loss: 1.9068 - acc: 0.3944 - val_loss: 1.7733 - val_acc: 0.5067\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.88858 to 1.77326, saving model to model.h5\n",
      "Epoch 13/100\n",
      "890/890 [==============================] - 1s 653us/step - loss: 1.8191 - acc: 0.4337 - val_loss: 1.6625 - val_acc: 0.5336\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.77326 to 1.66250, saving model to model.h5\n",
      "Epoch 14/100\n",
      "890/890 [==============================] - 1s 670us/step - loss: 1.7363 - acc: 0.4528 - val_loss: 1.6687 - val_acc: 0.5650\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.66250\n",
      "Epoch 15/100\n",
      "890/890 [==============================] - 1s 653us/step - loss: 1.6755 - acc: 0.4753 - val_loss: 1.5374 - val_acc: 0.5471\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.66250 to 1.53740, saving model to model.h5\n",
      "Epoch 16/100\n",
      "890/890 [==============================] - 1s 683us/step - loss: 1.5750 - acc: 0.5270 - val_loss: 1.5294 - val_acc: 0.5381\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.53740 to 1.52940, saving model to model.h5\n",
      "Epoch 17/100\n",
      "890/890 [==============================] - 1s 860us/step - loss: 1.4945 - acc: 0.5427 - val_loss: 1.7462 - val_acc: 0.5471\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.52940\n",
      "Epoch 18/100\n",
      "890/890 [==============================] - 1s 893us/step - loss: 1.6489 - acc: 0.5157 - val_loss: 1.4473 - val_acc: 0.5695\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.52940 to 1.44728, saving model to model.h5\n",
      "Epoch 19/100\n",
      "890/890 [==============================] - 1s 673us/step - loss: 1.4711 - acc: 0.5472 - val_loss: 1.4740 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.44728\n",
      "Epoch 20/100\n",
      "890/890 [==============================] - 1s 633us/step - loss: 1.3893 - acc: 0.5719 - val_loss: 1.3442 - val_acc: 0.5740\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.44728 to 1.34420, saving model to model.h5\n",
      "Epoch 21/100\n",
      "890/890 [==============================] - 1s 712us/step - loss: 1.3091 - acc: 0.5843 - val_loss: 1.2332 - val_acc: 0.6278\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.34420 to 1.23323, saving model to model.h5\n",
      "Epoch 22/100\n",
      "890/890 [==============================] - 1s 839us/step - loss: 1.2616 - acc: 0.6101 - val_loss: 1.2381 - val_acc: 0.6233\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.23323\n",
      "Epoch 23/100\n",
      "890/890 [==============================] - 1s 933us/step - loss: 1.2248 - acc: 0.6169 - val_loss: 1.2059 - val_acc: 0.6771\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.23323 to 1.20589, saving model to model.h5\n",
      "Epoch 24/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 1.1953 - acc: 0.6416 - val_loss: 1.2150 - val_acc: 0.6502\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.20589\n",
      "Epoch 25/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 1.1242 - acc: 0.6461 - val_loss: 1.1390 - val_acc: 0.6816\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.20589 to 1.13902, saving model to model.h5\n",
      "Epoch 26/100\n",
      "890/890 [==============================] - 1s 951us/step - loss: 1.1044 - acc: 0.6551 - val_loss: 1.0748 - val_acc: 0.6861\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.13902 to 1.07479, saving model to model.h5\n",
      "Epoch 27/100\n",
      "890/890 [==============================] - 1s 685us/step - loss: 1.0599 - acc: 0.6899 - val_loss: 1.0677 - val_acc: 0.6951\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.07479 to 1.06769, saving model to model.h5\n",
      "Epoch 28/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 1.0833 - acc: 0.6775 - val_loss: 1.1667 - val_acc: 0.6771\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.06769\n",
      "Epoch 29/100\n",
      "890/890 [==============================] - 1s 843us/step - loss: 1.0778 - acc: 0.6730 - val_loss: 1.0653 - val_acc: 0.7040\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.06769 to 1.06534, saving model to model.h5\n",
      "Epoch 30/100\n",
      "890/890 [==============================] - 1s 919us/step - loss: 0.9870 - acc: 0.7101 - val_loss: 1.1682 - val_acc: 0.6726\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.06534\n",
      "Epoch 31/100\n",
      "890/890 [==============================] - 1s 676us/step - loss: 0.9938 - acc: 0.7067 - val_loss: 1.0287 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.06534 to 1.02866, saving model to model.h5\n",
      "Epoch 32/100\n",
      "890/890 [==============================] - 1s 775us/step - loss: 0.9525 - acc: 0.6989 - val_loss: 0.9879 - val_acc: 0.7175\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.02866 to 0.98788, saving model to model.h5\n",
      "Epoch 33/100\n",
      "890/890 [==============================] - 1s 847us/step - loss: 0.9168 - acc: 0.7236 - val_loss: 0.9502 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.98788 to 0.95023, saving model to model.h5\n",
      "Epoch 34/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.9679 - acc: 0.7000 - val_loss: 1.0772 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.95023\n",
      "Epoch 35/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.9543 - acc: 0.7180 - val_loss: 0.9976 - val_acc: 0.7399\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.95023\n",
      "Epoch 36/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.8675 - acc: 0.7461 - val_loss: 0.9970 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.95023\n",
      "Epoch 37/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.8668 - acc: 0.7191 - val_loss: 0.9335 - val_acc: 0.7444\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.95023 to 0.93354, saving model to model.h5\n",
      "Epoch 38/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.9012 - acc: 0.7337 - val_loss: 1.0215 - val_acc: 0.7175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: val_loss did not improve from 0.93354\n",
      "Epoch 39/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.8903 - acc: 0.7270 - val_loss: 0.9758 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.93354\n",
      "Epoch 40/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.8824 - acc: 0.7247 - val_loss: 1.0317 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.93354\n",
      "Epoch 41/100\n",
      "890/890 [==============================] - 1s 869us/step - loss: 0.8364 - acc: 0.7382 - val_loss: 0.9407 - val_acc: 0.7444\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.93354\n",
      "Epoch 42/100\n",
      "890/890 [==============================] - 1s 853us/step - loss: 0.8012 - acc: 0.7483 - val_loss: 0.9225 - val_acc: 0.7489\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.93354 to 0.92254, saving model to model.h5\n",
      "Epoch 43/100\n",
      "890/890 [==============================] - 1s 986us/step - loss: 0.7540 - acc: 0.7607 - val_loss: 0.8914 - val_acc: 0.7668\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.92254 to 0.89139, saving model to model.h5\n",
      "Epoch 44/100\n",
      "890/890 [==============================] - 1s 822us/step - loss: 0.7442 - acc: 0.7865 - val_loss: 0.9480 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.89139\n",
      "Epoch 45/100\n",
      "890/890 [==============================] - 1s 865us/step - loss: 0.7495 - acc: 0.7809 - val_loss: 0.9811 - val_acc: 0.7489\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.89139\n",
      "Epoch 46/100\n",
      "890/890 [==============================] - 1s 847us/step - loss: 0.9223 - acc: 0.7348 - val_loss: 1.0106 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.89139\n",
      "Epoch 47/100\n",
      "890/890 [==============================] - 1s 840us/step - loss: 0.8077 - acc: 0.7517 - val_loss: 0.9555 - val_acc: 0.7489\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.89139\n",
      "Epoch 48/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.7189 - acc: 0.7809 - val_loss: 0.9470 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.89139\n",
      "Epoch 49/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.7242 - acc: 0.7730 - val_loss: 0.9255 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.89139\n",
      "Epoch 50/100\n",
      "890/890 [==============================] - 1s 943us/step - loss: 0.6865 - acc: 0.7775 - val_loss: 0.9171 - val_acc: 0.7668\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.89139\n",
      "Epoch 51/100\n",
      "890/890 [==============================] - 1s 981us/step - loss: 0.6676 - acc: 0.7910 - val_loss: 0.9558 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.89139\n",
      "Epoch 52/100\n",
      "890/890 [==============================] - 1s 967us/step - loss: 0.6539 - acc: 0.7921 - val_loss: 0.9455 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.89139\n",
      "Epoch 53/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.6455 - acc: 0.7966 - val_loss: 0.9645 - val_acc: 0.7668\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.89139\n",
      "Epoch 54/100\n",
      "890/890 [==============================] - 1s 984us/step - loss: 0.7996 - acc: 0.7562 - val_loss: 1.2623 - val_acc: 0.6771\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.89139\n",
      "Epoch 55/100\n",
      "890/890 [==============================] - 1s 965us/step - loss: 1.0676 - acc: 0.6719 - val_loss: 1.1180 - val_acc: 0.6682\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.89139\n",
      "Epoch 56/100\n",
      "890/890 [==============================] - 1s 983us/step - loss: 0.8359 - acc: 0.7360 - val_loss: 0.9421 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.89139\n",
      "Epoch 57/100\n",
      "890/890 [==============================] - 1s 1000us/step - loss: 0.7148 - acc: 0.7730 - val_loss: 0.9066 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.89139\n",
      "Epoch 58/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.6939 - acc: 0.7764 - val_loss: 0.9644 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.89139\n",
      "Epoch 59/100\n",
      "890/890 [==============================] - 1s 980us/step - loss: 0.6915 - acc: 0.7798 - val_loss: 0.9785 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.89139\n",
      "Epoch 60/100\n",
      "890/890 [==============================] - 1s 985us/step - loss: 0.6778 - acc: 0.7831 - val_loss: 0.9040 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.89139\n",
      "Epoch 61/100\n",
      "890/890 [==============================] - 1s 982us/step - loss: 0.6979 - acc: 0.7775 - val_loss: 0.9278 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.89139\n",
      "Epoch 62/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.6721 - acc: 0.7921 - val_loss: 0.8937 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.89139\n",
      "Epoch 63/100\n",
      "890/890 [==============================] - 1s 992us/step - loss: 0.6460 - acc: 0.8000 - val_loss: 0.8820 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.89139 to 0.88205, saving model to model.h5\n",
      "Epoch 64/100\n",
      "890/890 [==============================] - 1s 969us/step - loss: 0.6272 - acc: 0.8090 - val_loss: 0.8455 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.88205 to 0.84552, saving model to model.h5\n",
      "Epoch 65/100\n",
      "890/890 [==============================] - 1s 954us/step - loss: 0.6037 - acc: 0.8225 - val_loss: 0.9980 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.84552\n",
      "Epoch 66/100\n",
      "890/890 [==============================] - 1s 987us/step - loss: 0.5813 - acc: 0.8315 - val_loss: 0.8939 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.84552\n",
      "Epoch 67/100\n",
      "890/890 [==============================] - 1s 993us/step - loss: 0.5907 - acc: 0.8135 - val_loss: 0.9624 - val_acc: 0.7444\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.84552\n",
      "Epoch 68/100\n",
      "890/890 [==============================] - 1s 981us/step - loss: 0.6107 - acc: 0.7966 - val_loss: 0.8610 - val_acc: 0.7892\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.84552\n",
      "Epoch 69/100\n",
      "890/890 [==============================] - 1s 986us/step - loss: 0.5654 - acc: 0.8124 - val_loss: 1.1088 - val_acc: 0.7444\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.84552\n",
      "Epoch 70/100\n",
      "890/890 [==============================] - 1s 990us/step - loss: 0.5793 - acc: 0.8112 - val_loss: 0.8183 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.84552 to 0.81833, saving model to model.h5\n",
      "Epoch 71/100\n",
      "890/890 [==============================] - 1s 966us/step - loss: 0.5203 - acc: 0.8404 - val_loss: 1.1473 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.81833\n",
      "Epoch 72/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.5246 - acc: 0.8303 - val_loss: 0.9502 - val_acc: 0.7892\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.81833\n",
      "Epoch 73/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.5522 - acc: 0.8225 - val_loss: 0.9516 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.81833\n",
      "Epoch 74/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.5268 - acc: 0.8371 - val_loss: 0.9747 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.81833\n",
      "Epoch 75/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.6686 - acc: 0.8213 - val_loss: 1.1193 - val_acc: 0.7130\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.81833\n",
      "Epoch 76/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.7565 - acc: 0.7652 - val_loss: 0.9856 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.81833\n",
      "Epoch 77/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.6541 - acc: 0.8067 - val_loss: 0.9473 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.81833\n",
      "Epoch 78/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.5560 - acc: 0.8258 - val_loss: 0.9900 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.81833\n",
      "Epoch 79/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.5492 - acc: 0.8315 - val_loss: 0.9132 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.81833\n",
      "Epoch 80/100\n",
      "890/890 [==============================] - 1s 975us/step - loss: 0.5205 - acc: 0.8258 - val_loss: 0.9061 - val_acc: 0.7848\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.81833\n",
      "Epoch 81/100\n",
      "890/890 [==============================] - 1s 915us/step - loss: 0.4988 - acc: 0.8337 - val_loss: 0.8723 - val_acc: 0.8027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00081: val_loss did not improve from 0.81833\n",
      "Epoch 82/100\n",
      "890/890 [==============================] - 1s 848us/step - loss: 0.5449 - acc: 0.8292 - val_loss: 0.9366 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.81833\n",
      "Epoch 83/100\n",
      "890/890 [==============================] - 1s 866us/step - loss: 0.4687 - acc: 0.8562 - val_loss: 0.8894 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.81833\n",
      "Epoch 84/100\n",
      "890/890 [==============================] - 1s 841us/step - loss: 0.4835 - acc: 0.8573 - val_loss: 0.9760 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.81833\n",
      "Epoch 85/100\n",
      "890/890 [==============================] - 1s 879us/step - loss: 0.4509 - acc: 0.8427 - val_loss: 1.0505 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.81833\n",
      "Epoch 86/100\n",
      "890/890 [==============================] - 1s 829us/step - loss: 0.4786 - acc: 0.8382 - val_loss: 1.0083 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.81833\n",
      "Epoch 87/100\n",
      "890/890 [==============================] - 1s 842us/step - loss: 0.4492 - acc: 0.8517 - val_loss: 0.9287 - val_acc: 0.7848\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.81833\n",
      "Epoch 88/100\n",
      "890/890 [==============================] - 1s 900us/step - loss: 0.4372 - acc: 0.8539 - val_loss: 1.0942 - val_acc: 0.7892\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.81833\n",
      "Epoch 89/100\n",
      "890/890 [==============================] - 1s 922us/step - loss: 0.4302 - acc: 0.8517 - val_loss: 1.0769 - val_acc: 0.7892\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.81833\n",
      "Epoch 90/100\n",
      "890/890 [==============================] - 1s 912us/step - loss: 0.4298 - acc: 0.8652 - val_loss: 0.9968 - val_acc: 0.7848\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.81833\n",
      "Epoch 91/100\n",
      "890/890 [==============================] - 1s 863us/step - loss: 0.4397 - acc: 0.8539 - val_loss: 1.0710 - val_acc: 0.7668\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.81833\n",
      "Epoch 92/100\n",
      "890/890 [==============================] - 1s 866us/step - loss: 0.4197 - acc: 0.8730 - val_loss: 0.9354 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.81833\n",
      "Epoch 93/100\n",
      "890/890 [==============================] - 1s 823us/step - loss: 0.4294 - acc: 0.8517 - val_loss: 0.9565 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.81833\n",
      "Epoch 94/100\n",
      "890/890 [==============================] - 1s 819us/step - loss: 0.4081 - acc: 0.8697 - val_loss: 0.9542 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.81833\n",
      "Epoch 95/100\n",
      "890/890 [==============================] - 1s 833us/step - loss: 0.4506 - acc: 0.8494 - val_loss: 1.1392 - val_acc: 0.7489\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.81833\n",
      "Epoch 96/100\n",
      "890/890 [==============================] - 1s 973us/step - loss: 0.5296 - acc: 0.8337 - val_loss: 0.9589 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.81833\n",
      "Epoch 97/100\n",
      "890/890 [==============================] - 1s 865us/step - loss: 0.4552 - acc: 0.8449 - val_loss: 0.8954 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.81833\n",
      "Epoch 98/100\n",
      "890/890 [==============================] - 1s 927us/step - loss: 0.3887 - acc: 0.8831 - val_loss: 0.8981 - val_acc: 0.8117\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.81833\n",
      "Epoch 99/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.3836 - acc: 0.8753 - val_loss: 0.9517 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.81833\n",
      "Epoch 100/100\n",
      "890/890 [==============================] - 1s 1ms/step - loss: 0.3808 - acc: 0.8640 - val_loss: 0.9094 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.81833\n"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "hist = model.fit(train_X, train_Y, epochs = 100, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    test_word = word_tokenize(clean)\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "    print(test_word)\n",
    "    #Check for unknown words\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    "    x = padding_doc(test_ls, max_length)\n",
    "    pred = model.predict_proba(x)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "    predictions = pred[0]\n",
    "    classes = np.array(classes)\n",
    "    ids = np.argsort(-predictions)\n",
    "    classes = classes[ids]\n",
    "    predictions = -np.sort(-predictions)\n",
    "\n",
    "    for i in range(pred.shape[1]):\n",
    "        print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'want', 'to', 'get', 'in', 'touch']\n",
      "[[1.60746351e-02 1.78172905e-02 7.64280921e-05 2.40478739e-02\n",
      "  4.42507714e-02 3.03787410e-01 3.71068040e-06 2.02602092e-02\n",
      "  1.07709114e-02 3.75265479e-01 8.54797382e-03 1.51545510e-01\n",
      "  1.11071905e-02 5.02497656e-04 2.63005350e-04 6.48355999e-05\n",
      "  1.54846246e-04 8.20696951e-05 1.00626657e-02 7.10455875e-04\n",
      "  4.60428558e-03]]\n",
      "contact.contact has confidence = 0.37526548\n",
      "faq.biz_new has confidence = 0.3037874\n",
      "commonQ.bot has confidence = 0.15154551\n",
      "faq.application_process has confidence = 0.04425077\n",
      "faq.apply_register has confidence = 0.024047874\n",
      "commonQ.how has confidence = 0.02026021\n",
      "commonQ.name has confidence = 0.01781729\n",
      "commonQ.not_giving has confidence = 0.016074635\n",
      "commonQ.assist has confidence = 0.0111071905\n",
      "commonQ.wait has confidence = 0.010770911\n",
      "faq.bad_service has confidence = 0.010062666\n",
      "commonQ.query has confidence = 0.008547974\n",
      "commonQ.just_details has confidence = 0.0046042856\n",
      "faq.approval_time has confidence = 0.0007104559\n",
      "faq.biz_simpler has confidence = 0.00050249766\n",
      "faq.banking_option_missing has confidence = 0.00026300535\n",
      "faq.aadhaar_missing has confidence = 0.00015484625\n",
      "faq.borrow_use has confidence = 8.2069695e-05\n",
      "faq.address_proof has confidence = 7.642809e-05\n",
      "faq.borrow_limit has confidence = 6.48356e-05\n",
      "faq.biz_category_missing has confidence = 3.7106804e-06\n"
     ]
    }
   ],
   "source": [
    "text = \"i want to get in touch\"\n",
    "pred = predictions(text)\n",
    "print(pred)\n",
    "get_final_output(pred, unique_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
